# -*- coding: utf-8 -*-
"""threatscoring and anomly dtection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZW8__MzizkTxSUe5MV-jJqW7oxfFQfw7
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import gspread
from gspread_dataframe import set_with_dataframe
from google.colab import auth
from google.auth import default
import time

# Authenticate and connect to Google Sheets
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Sheet details
spreadsheet_name = "supervised_dataset"
worksheet_name = "dummy data genration "  # Default name if not renamed

# Open Google Sheet and worksheet
spreadsheet = gc.open(spreadsheet_name)
worksheet = spreadsheet.worksheet(worksheet_name)

# Function to classify and update sheet with threat_score and classification
def classify_and_update(interval=4):
    while True:
        print("Fetching data...")
        data = worksheet.get_all_records()
        df = pd.DataFrame(data)

        if df.empty:
            print("No data yet.")
            time.sleep(interval)
            continue

        # Ensure all required columns are present
        required_columns = [
            'inter_api_access_duration(sec)', 'api_access_uniqueness',
            'sequence_length(count)', 'vsession_duration(min)',
            'ip_type', 'num_sessions', 'num_users',
            'num_unique_apis', 'source'
        ]

        if not all(col in df.columns for col in required_columns):
            print("Missing one or more required columns.")
            time.sleep(interval)
            continue

        # Drop rows with missing required data
        df = df.dropna(subset=required_columns)

        # Copy for model training
        X = df[required_columns].copy()

        # Encode 'ip_type' and 'source'
        X['ip_type'] = X['ip_type'].astype('category').cat.codes
        X['source'] = X['source'].astype('category').cat.codes

        # Standard scaling
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Fallback: generate random threat scores if classification not yet available
        if 'classification' not in df.columns or df['classification'].nunique() < 2:
            df['threat_score'] = np.round(np.random.rand(len(df)), 5)
        else:
            # Prepare labels
            label_encoder = LabelEncoder()
            y = label_encoder.fit_transform(df['classification'])

            # Train/test split
            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

            # Train classifier
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X_train, y_train)

            # Predict threat scores
            df['threat_score'] = np.round(model.predict_proba(X_scaled)[:, 1], 5)

        # Add classification based on threshold
        df['classification'] = df['threat_score'].apply(lambda x: 'Outlier' if x > 0.5 else 'Normal')

        # Overwrite sheet with updated data
        worksheet.clear()
        set_with_dataframe(worksheet, df, include_index=False)
        print("Updated threat_score and classification for all rows.")
        time.sleep(interval)

# Start monitoring
classify_and_update(interval=660)